#training parameters
epochs : 100 # optimisation epochs
batch_size : 64 # batch size
lr: 0.01 # learning rate
momentum: 0.9

#manifold/signal parameters
order: 2 # order to which to compute the directional derivatives
inner_product_features: False
vector_diffusion: False
scalar_diffusion: True
frac_sampled_nb: -1 # fraction of neighbours to sample for gradient computation (if -1 then all neighbours)
include_positions: False # include positions as features (warning: this is untested!)
include_self: True # include vector at the center of feature

# network parameters
dropout: 0. # dropout in the MLP
hidden_channels: [32] # number of hidden channels
out_channels: 3 # number of output channels (if null, then =hidden_channels)
vec_norm: False # normalise features at each order of derivatives
bias: True # learn bias parameters in MLP
batch_norm: True # batch normalisation
emb_norm: False # spherical output

# orthogonal gradient
global_align: False # align global embeddings
final_grad: True # only compute orthogonal gradient at end of batch
positional_grad: True # include positions for orthogonal gradient computation
vector_grad: True # use vectors for orthogonal gradient computation
derivative_grad: True # use derivatives of vectors for orthogonal gradient computation
gauge_grad: True # use the vectors perpendicular to gauges for gradient computation

# other params
seed: 0 # seed for reproducibility